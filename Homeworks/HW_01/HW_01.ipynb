{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание №1: Полносвязные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пожалуйста, заполните имя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDENT_NAME = \"FirstName LastName\" # For example, Fedor Petriaikin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Включает режим проверки\n",
    "from importlib import reload\n",
    "\n",
    "try:\n",
    "    import test_lib\n",
    "    TEST_MODE = True\n",
    "except:\n",
    "    TEST_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Не удаляйте клетки из этого jupyter-notebok! Это затруднит проверку!**\n",
    "\n",
    "В этом задании вам предстоит самостоятельно самостоятельно реализовать простую полносвязную нейронную сеть, процедуру обучения. Правильность нашей реализации мы проверим на примере задачи распознавания рукописных цифр (датасет MNIST)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый тип слоя мы будем реализовывать при помощи класса, который будет поддерживать три функции:\n",
    "1. forward(X) - применяет функцию, реализуемую слоем, к входной матрице (X), X.shape = N x features_size, N - размер батча\n",
    "2. backward(dLdy) - которая по $\\frac{\\partial L}{\\partial y}$ будет вычислять $\\frac{\\partial L}{\\partial x}$ и обновлять внутри себя $\\frac{\\partial L}{\\partial w}$, dLdy.shape - N x 1 = градиент по каждому из объектов батча\n",
    "3. step(learning_rate), которая будет обновлять веса в слое\n",
    "\n",
    "Чтобы не применять функцию к каждому объекту в отдельности, мы будем подавать на вход слою матрицу размера (N, d), где N — количество объектов (размер batch-а), а d (то же, что и input_size, features_count) — размерность каждого объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "**Требуется реализовать слои (в layers.py):**  \n",
    "1. Linear. В инициализацию весов следует сделать из N(0, 0.01)\n",
    "2. Sigmoid, ReLU, ELU, Tanh - нелинейности\n",
    "3. SoftMax_NLLLoss. Обратите внимание, что это особый слой - он совмещает в себе и слой-предиктор, и функцию ошибок. Это нужно, чтобы упросить расчет градиента с помощью трюка log-sum-exp (позволяет избежать численных неустойчивостей):  \n",
    "https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\n",
    "4. MSE_Error - loss-функция, считает dL/dy для запуска backprop.\n",
    "5. NeuralNetwork - класс, объединяющий слои\n",
    "\n",
    "**Обратите внимание**\n",
    "1. Изменять интерфейс классов в layers.py **не следует**, т.к. иначе тестирующий код не сможет выполниться\n",
    "2. Важным требованием к реализации является векторизация всех слоев: **все операции должны быть сведены к матричным**, не должно быть циклов. Это значительно уменьшает временные затраты.\n",
    "3. Отличная визуализация работы полносвязных нейронных сетей: http://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка задания 1\n",
    "Нужно написать свой код для проверки и пройти тесты кода, который не предоставляется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверка 1 - численный градиент**\n",
    "\n",
    "1. Релизуйте функцию проверки численного градиента. Для этого для каждой переменной, по которой считается градиент, надо вычислить численный градиент: $f'(x) \\approx \\frac{f(x+\\epsilon)-f(x-\\epsilon)}{2\\epsilon}$. Функция должна возвращать максимальное абсолютное отклонение аналитического градиента от численного. В качестве $\\epsilon$ рекомендуется взять $10^{-6}$. При правильной реализации максимальное отличие будет иметь порядок $10^{-8}-10^{-6}$.\n",
    "\n",
    "2. Протестируйте линейный слой при помощи реализованной функции check_gradient: $\\frac{\\partial L}{\\partial x}$ и $\\frac{\\partial L}{\\partial w}$.\n",
    "\n",
    "3. По аналогии, протестируйте SoftMax_NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "def check_gradient(func, X, gradient):\n",
    "    '''\n",
    "    Computes numerical gradient and compares it with analytcal.\n",
    "    func: callable, function of which gradient we are interested. Example call: func(X)\n",
    "    X: np.array of size (n x m)\n",
    "    gradient: np.array of size (n x m)\n",
    "    Returns: maximum absolute diviation between numerical gradient and analytical.\n",
    "    '''\n",
    "    #### YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# 2. Проверка слоя Linear\n",
    "lin = layers.Linear(10, 2)\n",
    "X = np.ones((5, 10))\n",
    "y = lin.forward(X)\n",
    "\n",
    "func = lambda x: lin.forward(x).sum() # Аналог функции ошибок\n",
    "dLdy = np.ones((5, 2)) # Очевидно, что производная суммы по всем y = 1\n",
    "gradient = lin.backward(dLdy)\n",
    "print(\"Check grad linear: {}\".format(check_gradient(func, X, gradient)))\n",
    "\n",
    "#3. Проверка SoftMax_NLLLoss\n",
    "#### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка вашей реализации check_gradient и Linear\n",
    "# Не изменяйте эти ячейки!\n",
    "if TEST_MODE:\n",
    "    try:\n",
    "        test_lib.test_grad_checker(test_lib.check_gradient)\n",
    "        test_lib.check_grads(layers.Linear, layers.Sigmoid, layers.SoftMax_NLLLoss, layers.Tanh)\n",
    "    except Exception as e:\n",
    "        print(\"!!>> Tasks grad_check failed:\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверка 2**  \n",
    "Следующий код должен выполниться и дать корректную классификацию объектов. Это нужно, чтобы проверить интерфейс классов из layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data.npz')\n",
    "X, y = data['arr_0'], data['arr_1']\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc = [layers.Linear(2, 3), layers.Sigmoid(), layers.Linear(3, 3), layers.SoftMax_NLLLoss()]\n",
    "network = layers.NeuralNetwork(arc)\n",
    "\n",
    "def learn_network(network, X, y, learning_rate=0.1, iterations=16000):\n",
    "    for i in range(iterations):\n",
    "        prediction = network.forward(X)\n",
    "        network.backward(y)\n",
    "        network.step(learning_rate)\n",
    "\n",
    "learn_network(network, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mesh_with_scatter(X, y, pred_func, N=50, additional=0.15):\n",
    "    '''\n",
    "    pred_func(mesh) - returns classes from mesh grid\n",
    "    '''\n",
    "    delta_x = (np.max(X[:, 0])-np.min(X[:, 0]))*additional\n",
    "    delta_y = (np.max(X[:, 1])-np.min(X[:, 1]))*additional\n",
    "    \n",
    "    x_grid = np.linspace(np.min(X[:, 0])-delta_x, np.max(X[:, 0])+delta_x, N)\n",
    "    y_grid = np.linspace(np.min(X[:, 1])-delta_y, np.max(X[:, 1])+delta_y, N)\n",
    "    X_mesh, Y_mesh = np.meshgrid(x_grid, y_grid)\n",
    "    mesh = np.dstack((X_mesh, Y_mesh)).reshape(N*N, 2)\n",
    "    \n",
    "    predictions = pred_func(mesh)\n",
    "    \n",
    "    plt.scatter(mesh[:, 0], mesh[:, 1], c=predictions, alpha=0.2)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mesh_with_scatter(X, y, lambda x: np.argmax(network.forward(x), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "1. Попробуйте разные темпы обучения (6-7 значений в пределах 0.01 - 20, с логарифмическим масштабом).  \n",
    "Как меняется сходимость и почему? Нарисуйте график оптимального значения функции потерь для различных значений learning_rate (для этого реализуйте в нейронной сети и ее слоях функцию get_loss)\n",
    "2. Решите поставленную выше задачу как задачу регрессии с MSE. Изменилась ли разделяющая поверхность? Нарисуйте ее, с помощью plot_mesh_with_scatter.\n",
    "\n",
    "**На это задание нет специальных тестов, оно проверяется вручную**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Распознование рукописных цифр\n",
    "Ниже приведен код для загрузки датасета и визуализация нескольких объектов из него"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не изменяйте имена в этой ячейке! Они нужны для тестов\n",
    "transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,)) # Нормализация данных\n",
    "                    ])\n",
    "train_dataset = MNIST('.', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST('.', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in train_loader:\n",
    "    X = X.view(X.shape[0], -1)\n",
    "    X = X.numpy() ### Converts torch.Tensor to numpy array\n",
    "    y = y.numpy()\n",
    "\n",
    "plt.figure(figsize=(6, 7))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(X[i].reshape(28, 28), cmap=plt.cm.Greys_r)\n",
    "    plt.title(y[i])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "Обучите полносвязную нейронную сеть с архитектурой 784-100-100-10 и сигмоидой в качестве нелинейности (объект создан ниже).  \n",
    "Какую точность классификации удалось получить? Нарисуйте график сходимости на обучающей и тестовой выборках. В качестве темпа обучения (learning rate) возьмите 0.4. Число эпох выберете по своему усмотрению.\n",
    "\n",
    "**Обратите внимание**  \n",
    "1. Итерироваться по данным можно с помощью выражения *for X, y in some_loader* При этом X, y - будут иметь тип torch.Tensor. Чтобы конвертировать их в numpy-массивы нужно вызвать функцию .numpy()\n",
    "2. Каждый объект из X имеет размерность 1x28x28. Поэтому перед .numpy() нужно вызвать *X = X.view(X.shape[0], -1)*, чтобы получить одномерный массив\n",
    "3. Качество на тесте (масксимальное среди эпох) должно быть не хуже **0.972**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(network, epochs, learning_rate, train=train_loader, test=test_loader):\n",
    "    ### YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TEST_MODE:\n",
    "    epochs_count = 0 # Введите сюда число эпох\n",
    "\n",
    "    neural_network = layers.NeuralNetwork([layers.Linear(784, 100), layers.Sigmoid(), layers.Linear(100, 100), \n",
    "                                           layers.Sigmoid(), layers.Linear(100, 10), layers.SoftMax_NLLLoss()])\n",
    "    train_nn(neural_network, epochs_count, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка задания 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка качества нейронной сети\n",
    "if TEST_MODE:\n",
    "    test_lib.train_task3(learning_rate, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "Обучите нейронную сеть как в задании 3, но используйте в качестве нелинейности ELU и ReLU. Качество должно быть не менее **0.977** в каждом случае (можно подбирать learning_rate, число эпох, коэффициенты в ELU/ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подбирайте параметры с помощью этих параметров\n",
    "learning_rate_elu  = 0\n",
    "learning_rate_relu = 0\n",
    "\n",
    "epochs_elu = 0\n",
    "epochs_relu = 0\n",
    "\n",
    "a_elu = 0\n",
    "a_relu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TEST_MODE:\n",
    "    ### YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка задания 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка качества нейронной сети (ELU)\n",
    "if TEST_MODE:\n",
    "    test_lib.train_task4_elu(a_elu, learning_rate_elu, epochs_elu, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка качества нейронной сети (ReLU)\n",
    "if TEST_MODE:\n",
    "    test_lib.train_task4_relu(a_relu, learning_rate_relu, epochs_relu, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "Примените аугментацию:\n",
    "* Небольшие вращения (-15, 15)\n",
    "* Случайные сдвиги\n",
    "* Шум\n",
    "\n",
    "Какой прирост дают эти аугментации вместе и по отдельности? Напишите вывод. Постарайтесь добиться максимального качества, не изменяя размерности слоев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import shift, rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X - np.array, X.shape = batch_size x 1 x 28 x 28\n",
    "def add_noise(X, noise_std):\n",
    "    ### YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def random_shift(X, max_shift):\n",
    "    ### YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def random_rotation(X, max_rot=15):\n",
    "    ### YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Исправьте функцию train, чтобы она принимала augmentations - массив функций f(x),\n",
    "# которые производят преобразования-аугментации\n",
    "# Воспользуйтесь X.numpy(), а потом ndarray.view() для сокращения числа размеростей\n",
    "def train_nn_aug(network, epochs, learning_rate, augmentations, train=train_loader, test=test_loader):\n",
    "    ### YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проанализируйте различные аугментации\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В этих функциях нужно создать нейронную сеть и массив функций-аугментаторов, которые дают наилучший скор\n",
    "def create_best_nn():\n",
    "    '''\n",
    "    Returns NeuralNetwork object\n",
    "    '''\n",
    "    ### YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def create_best_augmentations():\n",
    "    '''\n",
    "    Returns array with aurmentation functions\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "# Здесь же можно задать learning_rate и число эпох\n",
    "learning_rate = 0.05\n",
    "epochs_count = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка задания 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка качества нейронной сети\n",
    "if TEST_MODE:\n",
    "    test_lib.train_task5(create_best_nn, create_best_augmentations, learning_rate, epochs_count,\n",
    "                         train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спасибо за выполнение заданий!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
