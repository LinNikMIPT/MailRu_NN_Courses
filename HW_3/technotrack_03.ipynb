{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ №3: Рекуррентные нейронные сети.  Обработка естественного языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пожалуйста, заполните имя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDENT_NAME = \"FirstName LastName\" # For example, Fedor Petriaikin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Не удаляйте клетки из этого jupyter-notebok! Это затруднит проверку!**\n",
    "\n",
    "В этом задании вам предстоит попробовать различные рекуррентные архитектуры для предсказания временного ряда и написания простейшего чат-бота.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1. Предсказание временного ряда"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "а) Напишите свою реализацию LSTM\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*hl9UVtgIcQkDIGD8VFykdw.png\" width=\"640\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кстати, на вход в pytorch-реализацию можно подавать последовательности разной длины.\n",
    "# См. torch.nn.utils.rnn.pack_padded_sequence() и torch.nn.utils.rnn.pack_sequence()\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        ## YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    # Пропускает через себя последовательность и выдает последний выход\n",
    "    # X: (seq_len, batch_size, input_size)\n",
    "    # hidden = (hidden, C): (batch_size, hidden_size)x2 - чтобы начать предсказания с какого-то момента\n",
    "    # Если None, инициализировать нулями\n",
    "    # Возвращает out, h, c, где (h, c) - выход и скрытое состояние после последнего элемента\n",
    "    # out - (seq_len, batch, hidden_size) - выходы каждого слоя\n",
    "    def forward(self, X, hidden=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "б) Напишите код обучения LSTM на представленных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График (*)\n",
    "%matplotlib inline\n",
    "dataset = pandas.read_csv('international-airline-passengers.csv', usecols=[1], delimiter=\";\", engine='python', skipfooter=3)\n",
    "plt.plot(dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовливаем данные\n",
    "dataset = dataset.values\n",
    "dataset = dataset.astype('float32')\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разбиваем данные на train / test\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Целевая переменная - сдвинутая последовательность значений входов\n",
    "# look_back - на сколько смещен test относительно train (seq_len)\n",
    "# look_forward - сколько нужно предсказать\n",
    "def create_dataset(dataset, look_back=1, look_forward=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1-look_forward):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back : (i + look_back + look_forward), 0])\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример подготовленных данных\n",
    "look_back = 5\n",
    "look_forward=1\n",
    "trainX, trainY = create_dataset(train, look_back, look_forward)\n",
    "testX, testY = create_dataset(test, look_back, look_forward)\n",
    "\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Берет данные из dataloader, сам их сдвигает, формируя обучающую выборку\n",
    "# В качестве функции ошибок возьмите MSELoss, оптимизатор - Adagrad\n",
    "# В функцию можно добавить дополнительные параметры (например, max_epochs, условия останова...)\n",
    "def TrainModel(model, dataloader, look_back=1, look_forward=1):\n",
    "    ## YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в) Попробуйте обучить сеть с разными параметрами look_back, look_forward. Также попробуйте использовать различные hidden_size. Выведите предсказания на график (\\*). Сделайте вывод.  \n",
    "*Обратите внимание*: \"Классический\", однонаправленный LSTM имеет размерность выхода (seq_len, batch, hidden_size). Но иногда нужно \"изменить\" размерность выхода - для этого можно обучить Linear: hidden_size x need_size  \n",
    "\n",
    "г) Сравните свой результат и результат с использованием nn.LSTM (на аналогичном числе эпох, hidden_size и т.п.)  \n",
    "\n",
    "д) Попробуйте использовать nn.GRU. Сравните результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2. RNN и обработка текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании мы будем использовать файл author-quote.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(777)\n",
    "torch.random.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [x.split('\\t')[1].strip() for x in open('author-quote.txt').readlines()]\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_len = []\n",
    "for l in map(len, lines):\n",
    "    lines_len.append(l)\n",
    "\n",
    "plt.hist(lines_len, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [l for l in filter(lambda x: len(x) <= 50, lines)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, sentences):\n",
    "        all_characters = set()\n",
    "        for line in sentences:\n",
    "            all_characters |= set(line)\n",
    "        all_characters = list(all_characters)+['<eos>', '<go>']\n",
    "        self.char_to_id = {x[1]:x[0] for x in enumerate(all_characters)}\n",
    "        self.id_to_char = {x[0]:x[1] for x in enumerate(all_characters)}\n",
    "        self.size = len(all_characters)\n",
    "\n",
    "    def encode(self, line):\n",
    "        return np.array([self.char_to_id[x] for x in line])\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        return ''.join([self.id_to_char[x] for x in tokens])\n",
    "    \n",
    "vocab = Vocabulary(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <go>, <eos> - специальные токены начала и конца последовательности, нужны для работы предсказаний\n",
    "class Quotes(Dataset):\n",
    "    def __init__(self, sentences, vocab):\n",
    "        # Construct vocabulary + EOS & GO tokens\n",
    "        self.sentences = sentences\n",
    "        self.vocab = vocab\n",
    "        self.go = self.vocab.char_to_id['<go>']\n",
    "        self.eos = self.vocab.char_to_id['<eos>']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.vocab.encode(self.sentences[idx])\n",
    "        _input = np.array([self.go]+tokens)\n",
    "        _output = np.array(tokens+[self.eos])\n",
    "        return _input, _output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для работы pack_padded_sequence\n",
    "def compose_batch(batch):\n",
    "    order = np.argsort([-len(x[0]) for x in batch])\n",
    "    lengths = np.array([len(x[0]) for x in batch])\n",
    "    go = torch.zeros(len(batch), lengths[order[0]]).long()\n",
    "    eos = torch.zeros(len(batch), lengths[order[0]]).long()\n",
    "    mask = torch.zeros(len(batch), lengths[order[0]]).long()\n",
    "    for i in range(len(batch)):\n",
    "        current_go, current_eos = batch[i]\n",
    "        go[i, :len(current_go)] = torch.tensor(current_go)\n",
    "        eos[i, :len(current_eos)] = torch.tensor(current_eos)\n",
    "        mask[i, :len(current_go)] = 1\n",
    "    mask = mask[order]\n",
    "    go = go[order]\n",
    "    eos = eos[order]\n",
    "    lengths = lengths[order]\n",
    "    return go, eos, mask, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Quotes(lines, vocab)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=64, collate_fn=compose_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "а) Напишите код класса Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Oracle(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_size=128, hidden_size=256, layers=2):\n",
    "        super(Oracle, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.layers = layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.generator = nn.GRU(embedding_size, hidden_size, layers, batch_first=False)\n",
    "        self.classifier = nn.Linear(hidden_size, vocabulary_size)\n",
    "\n",
    "    # 1) Генерируем эмбединги входных токенов (из _input), пакуем в pack_padded_sequence, используя lengths\n",
    "    # 2) Запускаем generator на эмбедингах, пакуем его вывод в pad_packed_sequence\n",
    "    # 3) Возвращаем предикты (пока без SoftMax) через self.classifier\n",
    "    def forward(self, _input, lengths):\n",
    "        ## YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    # Генерирует последовательность (когда уже обучен). Токены переводим в id с помощью vocab.char_to_id,\n",
    "    # обратно - с помощью vocab.id_to_char\n",
    "    # Первый токен - <go>, делаем embeding. Далее generator -- classifier -- softmax.\n",
    "    # Слово сэмплируем с помощью torch.distributions.Categorical, оно становится новым токеном\n",
    "    # Повторяем, пока не будет <eos> или не достигнем max_len\n",
    "    # Возвращаем - сгенерированную строку\n",
    "    def generate(self, vocab, max_len=100):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "б) Обучите модель. Приведите примеры сгенеррованных предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle = Oracle(vocab.size, embedding_size=32, hidden_size=64, layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтобы снизить колебания\n",
    "def moving_average(a, n=20) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar\n",
    "import tqdm\n",
    "\n",
    "for epoch in range(1000):\n",
    "    bar = progressbar.ProgressBar()\n",
    "    for i, (go, eos, mask, length) in enumerate(tqdm.tqdm(dataloader, total=len(dataloader))):\n",
    "        oracle.zero_grad()\n",
    "        prediction = oracle(go, length).transpose(0, 1).transpose(1, 2)\n",
    "        loss = (criterion(prediction, eos)*mask.float()).mean()\n",
    "        loss.backward()\n",
    "        [x.grad.clamp_(-1, 1) for x in oracle.parameters()]\n",
    "        optimizer.step()\n",
    "        losses.append(np.exp(loss.item()))\n",
    "        if i % 50 == 0:\n",
    "            clear_output(True)\n",
    "            plt.plot(losses, label='Train')\n",
    "            plt.plot(moving_average(losses), label='MA@20')\n",
    "            plt.xlabel('iteration')\n",
    "            plt.ylabel('perplexity')\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в) Обучить сеть с LSTM вместо GRU\n",
    "\n",
    "г) Сделать разбиение train-test и нарисровать графики зависимости перплексии от числа эпох\n",
    "\n",
    "д) Подобрать гиперпараметры, добиться лучшей перплексии чем с параметрами по умолчанию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Спасибо за выполнение заданий!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
